---
title: "Human Factors Experiments and Model Development for Multisensory Feedback"
institution: "OPPO Software User Research and Experience Design Department"
period: " Jul. 2023- Apr. 2024"
excerpt: "This project focuses on optimizing multisensory interaction experiences in smartphones, particularly the integration of tactile feedback. By exploring the synergistic relationships between visual, auditory, and tactile feedback, the project enhances user interaction experiences and efficiency. It specifically addresses high-frequency usage scenarios such as time selectors and camera zoom, focusing on compatibility, variability, and expressiveness to establish key design variables, thresholds and patterns. Methods include the establishment of audio and vibration data collection platforms, data collection and processing, and the use of psychophysical experiments to test and optimize multisensory feedback. These research findings have been applied to the latest version of the ColorOS system, providing important and effective scientific bases and practical guidance for multisensory design."
# img: "../images/portfolio/ChineseLab/ChineseLab_Intro.jpg"
collection: projects
---

<div>
    <p><br>
        <strong>Research Background</strong>
        <br>&nbsp;&nbsp;Tactile feedback, through direct contact with the skin, provides effective information without occupying screen space or causing disturbances in silent environments. Based on Wickens' Multiple Resource Theory, integrating tactile channels into multisensory displays can reduce sensory overload risks associated with single-sensory tasks, thereby decreasing interaction complexity and facilitating task completion. Further, enriched vibratory tactile feedback, combined with visual and auditory feedback, significantly enhances users' perception and awareness of interaction behaviors and environmental responses, increasing the immersion of the experience. Although current mobile devices predominantly use visual or auditory feedback, the demand for multisensory experiences is growing, prompting manufacturers to focus more on tactile feedback within multisensory experiences. There remains a gap between existing experiences and user expectations across various usage scenarios.
    </p>
</div>

<div>
    <p><br>
        <strong>Research Objectives</strong>
        <br>&nbsp;&nbsp;This project focuses on scenarios frequently used and highly mentioned by users, such as time selectors, camera zoom, dial pads, input methods, and notification alerts, aiming to define the synergistic relationships between visual, auditory, and tactile senses. The goal is to develop models for audio-tactile and audio-visual coordination, as well as key variable thresholds for multisensory design, providing a scientific basis for design. Additionally, user testing will be conducted in typical scenarios with different parameter combinations to formulate specific design schemes and recommendations. 
        <br>&nbsp;&nbsp;The research focuses on:
        <li><strong>Compatibility</strong>: Establishing optimal configurations of audio and vibration design variables that match the visual interface; exploring the synergistic effects of sound and vibration to optimize their compatibility; constructing multisensory match models.</li>
        <li><strong>Variability</strong>: Determining the rules and thresholds for changes in audio and vibration feedback during continuous operations in interactive scenarios.</li>
        <li><strong>Expressiveness</strong>: Analyzing various notification messages in different usage scenarios to identify design parameters related to vibration pattern recognition and user perception intensity levels, auditory and vibration thresholds.</li>
    </p>
</div>

<div>
    <p><br>
        <strong>Methods and Materials</strong>
        <li><strong>Platform Setup</strong>: Establish platforms for audio and vibration data collection.</li>
        <li><strong>Data Collection</strong>: Gather audio and vibration data through design workshops and reverse engineering competitive models.</li>
        <li><strong>Data Processing</strong>: Conduct data preprocessing and feature extraction to build datasets.</li>
        <li><strong>Subjective Evaluation</strong>: Define subjective evaluation dimensions through expert interviews, literature review, and PCA methods.</li>
        <li><strong>Psychophysical Experiments</strong>: Design and conduct experiments on compatibility, variability, and expressiveness.</li>
        <li><strong>Data Analysis</strong>: Analyze data using Python and other analytical tools, including exploratory, correlational, and significance analyses.</li>
    </p>
</div>

<div>
    <p><br>
        <strong>Main Outcomes</strong>
        <br>The project has produced analyses of user preferences in mobile interaction scenarios, offering qualitative and quantitative recommendations for optimizing multisensory feedback and ergonomics conclusions. These findings have been applied to the latest generation of the ColorOS system.
    </p>
</div>

<div>
    <p><br>
        <strong>My Contribution</strong>
        <li>Participated in project planning, literature review, technical investigation, user research (interviews, questionnaires)</li>
        <li>Set up audio-vibration experiment platform, and collected data from representative mobile models</li>
        <li>Developed experimental interaction prototypes and audio-vibration mapping program, design and conducted human factor experiments</li>
        <li>Visualized and analyzed data, and generated actionable insights, resulting in one CHI paper and one patent</li>  
    </p>
</div>

<!--
brief intro:
本项目聚焦于优化智能手机中的多感官交互体验，特别是触觉反馈的整合。通过研究视觉、听觉和触觉的协同关系，提升用户交互体验和交互效率。项目针对高频使用场景如时间选择器、相机变焦等，聚焦于匹配性、变化性、表达性三个方面，确定了关键设计变量和阈值。方法包括建立音效和振动采集平台，进行数据采集与处理，以及通过心理物理实验来测试和优化多感官反馈等。这些研究成果已被应用于 ColorOS 系统的最新版本，，为多感官设计提供了重要且有效的科学依据和实际指导。
“This project focuses on optimizing multisensory interaction experiences in smartphones, particularly the integration of tactile feedback. By exploring the synergistic relationships between visual, auditory, and tactile feedback, the project enhances user interaction experiences and efficiency. It specifically addresses high-frequency usage scenarios such as time selectors and camera zoom, focusing on compatibility, variability, and expressiveness to establish key design variables, thresholds and patterns. Methods include the establishment of audio and vibration data collection platforms, data collection and processing, and the use of psychophysical experiments to test and optimize multisensory feedback. These research findings have been applied to the latest version of the ColorOS system, providing important and effective scientific bases and practical guidance for multisensory design.”
-->

<!--

项目标题：多感官反馈人因实验与模型开发

项目背景：

触觉反馈通过与皮肤的直接接触，在不占用屏幕空间或不在静默环境中造成干扰的情况下，有效提供信息反馈。根据 Wickens 的多元资源理论，融入触觉通道的多感官信息显示可以减少单一感官任务可能导致的感官过载，降低交互难度，从而助力用户任务的完成。进一步地，丰富的振动触觉反馈，结合视觉和听觉反馈，通过多感官刺激的融合显著提升了用户对交互行为和环境反应的感知和认识，增强体验的沉浸感。尽管目前手机多通过视觉或听觉反馈，市场对多感官体验的需求不断提升，手机制造商开始更加关注触觉反馈在多感官体验中的作用。在各种使用场景中，现有体验与用户期望仍存在差距。

研究目标：

本项目聚焦于用户高频使用且提及率高的场景，如时间选择器、相机变焦、拨号盘、输入法、通知提醒等，旨在定义视觉、听觉与触觉间的协同关系。目标是开发出声振协同和视听协同的模型，以及多感官设计的关键变量阈值，为设计提供科学依据。此外，将针对典型场景下不同参数组合进行用户测试，形成具体设计方案和建议。研究聚焦于：
匹配性：确立视觉界面与音频、振动设计变量的最优配置；探究声音与振动的协同作用，优化其匹配性；构建多感官匹配模型。
变化性：确定交互场景中连续操作时，音效和振动反馈的变化规则和阈值。
表达性：分析各类消息通知在不同使用情景下：不同振动模式表达辨识度相关设计参数以及用户感知强度区分度、感知强度档位、听觉、振动阈值；

方法和材料：

平台搭建：建立音效和振动采集平台。
数据采集：通过设计工作坊和逆向采集竞品数据，获取音频、振动数据。
数据处理：进行数据预处理、特征提取，建立数据集。
主观评价维度：通过专家访谈、文献研究和 PCA 方法确定主观评价维度。
心理物理实验：设计并开展匹配性、变化性和表达性实验。
数据分析：使用 Python 及其他数据分析工具进行分析，包括探索性分析、相关性分析和显著性分析。

主要成果：

项目已产出基于手机交互场景的用户偏好分析，提供了定性与定量的多感官反馈的优化建议和人机工效学结论。这些成果已应用于新一代的 ColorOS 系统。

结论和影响：

本研究为多感官设计提供了科学依据，并推动了对人机交互中用户感知整合机制的理解。这些成果将促进更直观和沉浸式的用户界面设计，尤其是在虚拟现实和增强现实领域。

个人贡献：

➢ 研究移动交互中的多感官体验，探索协同方法、关键因素及阈值。
➢ 参与项目规划、文献回顾、技术调研、用户研究（访谈、问卷）。
➢ 搭建音效-振动实验平台，采集典型手机模型数据。
➢ 开发实验交互原型和音效-振动映射程序，设计并执行人因实验。
➢ 进行数据可视化和分析，产出 CHI 论文和专利。

ChatGPT
Project Title: Human Factors Experiments and Model Development for Multisensory Feedback

Project Background:

Tactile feedback, through direct contact with the skin, provides effective information without occupying screen space or causing disturbances in silent environments. Based on Wickens' Multiple Resource Theory, integrating tactile channels into multisensory displays can reduce sensory overload risks associated with single-sensory tasks, thereby decreasing interaction complexity and facilitating task completion. Further, enriched vibratory tactile feedback, combined with visual and auditory feedback, significantly enhances users' perception and awareness of interaction behaviors and environmental responses, increasing the immersion of the experience. Although current mobile devices predominantly use visual or auditory feedback, the demand for multisensory experiences is growing, prompting manufacturers to focus more on tactile feedback within multisensory experiences. There remains a gap between existing experiences and user expectations across various usage scenarios.

Research Objectives:

This project focuses on scenarios frequently used and highly mentioned by users, such as time selectors, camera zoom, dial pads, input methods, and notification alerts, aiming to define the synergistic relationships between visual, auditory, and tactile senses. The goal is to develop models for audio-tactile and audio-visual coordination, as well as key variable thresholds for multisensory design, providing a scientific basis for design. Additionally, user testing will be conducted in typical scenarios with different parameter combinations to formulate specific design schemes and recommendations. The research will focus on:

Match: Establishing optimal configurations of audio and vibration design variables that match the visual interface; exploring the synergistic effects of sound and vibration to optimize their compatibility; constructing multisensory match models.
Variability: Determining the rules and thresholds for changes in audio and vibration feedback during continuous operations in interactive scenarios.
Expressiveness: Analyzing various notification messages in different usage scenarios to identify design parameters related to vibration pattern recognition and user perception intensity levels, auditory and vibration thresholds.
Methods and Materials:

Platform Setup: Establish platforms for audio and vibration data collection.
Data Collection: Gather audio and vibration data through design workshops and reverse engineering competitive models.
Data Processing: Conduct data preprocessing and feature extraction to build datasets.
Subjective Evaluation: Define subjective evaluation dimensions through expert interviews, literature review, and PCA methods.
Psychophysical Experiments: Design and conduct experiments on compatibility, variability, and expressiveness.
Data Analysis: Analyze data using Python and other analytical tools, including exploratory, correlational, and significance analyses.
Main Outcomes:

The project has produced analyses of user preferences in mobile interaction scenarios, offering qualitative and quantitative recommendations for optimizing multisensory feedback and ergonomics conclusions. These findings have been applied to the latest generation of the ColorOS system.

Conclusions and Impact:

This research provides a scientific basis for multisensory design and advances understanding of user perception integration mechanisms in human-computer interaction. These outcomes are expected to foster more intuitive and immersive user interface designs, particularly in the fields of virtual reality and augmented reality.
-->
